{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Face Morph VAE Demo on UTKFace Dataset\n",
        "\n",
        "This notebook demonstrates training and evaluating a Variational Autoencoder (VAE) for face morphing on the UTKFace dataset.  \n",
        "We build a convolutional VAE that encodes 64x64 RGB face images into a 100-dimensional latent space and reconstructs them.  \n",
        "We visualize reconstructions, generate new samples, and perform latent space interpolation between face images.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Imports\n"
      ],
      "metadata": {
        "id": "xJoSVjXxXKZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "\n",
        "from model import VAE_UTKFace, ResidualBlock\n",
        "from utils import (\n",
        "    train_model,\n",
        "    show_reconstructions,\n",
        "    sample_from_latent,\n",
        "    show_interpolation_candidates,\n",
        "    interpolate_faces,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "0VBgGSP6XLgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "We load the UTKFace dataset images from a directory, resize them to 64x64, and convert to tensors.  \n",
        "A custom Dataset class handles loading images without labels since the VAE is unsupervised.  \n",
        "We split the dataset into 90% training and 10% validation.\n",
        "\n",
        "https://www.kaggle.com/datasets/jangedoo/utkface-new"
      ],
      "metadata": {
        "id": "chbXLCqbXXAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 68\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "encoding_size = 100\n",
        "input_channel = 3\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 50\n",
        "image_path = \"/kaggle/input/utkface-new/UTKFace\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class UTKFaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = [f for f in os.listdir(root_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
        "        image = PIL.Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "dataset = UTKFaceDataset(image_path, transform=transform)\n",
        "\n",
        "val_size = int(0.1 * len(dataset))\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n"
      ],
      "metadata": {
        "id": "v9AzxqlnXYJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model, Loss, Optimizer & Scheduler\n",
        "\n",
        "We instantiate the VAE model, define the BCEWithLogitsLoss for reconstruction, Adam optimizer, and a ReduceLROnPlateau scheduler.\n"
      ],
      "metadata": {
        "id": "-TYXRACUX6-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE_UTKFace(input_channel, encoding_size).to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n"
      ],
      "metadata": {
        "id": "Ps7-SCZ7X8zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "Train the model for 50 epochs.  \n",
        "We apply gradient clipping and cyclical Î² annealing for the KL term.  \n",
        "We plot training and validation loss curves after training.\n"
      ],
      "metadata": {
        "id": "ZnJdgZnjX_jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = train_model(model, num_epochs, train_dl, val_dl, loss_fn, optimizer, clip_norm=True, max_norm=50.0)\n",
        "\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wY5fptbdX_Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Reconstructions and Sampling\n",
        "\n",
        "We display reconstructions on the validation dataset and generate new samples from the latent space.\n"
      ],
      "metadata": {
        "id": "y9uoX32tYGX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_reconstructions(model, val_dl)\n",
        "sample_from_latent(model)\n"
      ],
      "metadata": {
        "id": "NndFKAQ8YHoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Space Interpolation\n",
        "\n",
        "We select two random faces from the validation set, visualize them, then perform interpolation between their latent vectors to morph one face into the other.\n"
      ],
      "metadata": {
        "id": "Q9_9rxjgYLgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_iter = iter(val_dl)\n",
        "val_batch = next(val_iter)\n",
        "img1, img2 = val_batch[0], val_batch[1]\n",
        "\n",
        "show_interpolation_candidates(img1, img2)\n",
        "\n",
        "interp_grid = interpolate_faces(model, img1, img2, steps=10)\n",
        "plt.figure(figsize=(14, 2))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Final Latent Interpolation After Training\")\n",
        "plt.imshow(interp_grid.permute(1, 2, 0).cpu())\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_tZJrhZkYM3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstructing Random Noise Inputs\n",
        "\n",
        "To further explore the model's generative capability, we feed it random noise as input and visualize the reconstructions.\n"
      ],
      "metadata": {
        "id": "hH4XJ72KYQIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = torch.randn(20, 3, 64, 64).to(device)\n",
        "show_reconstructions(model, sample_input=sample_input)\n"
      ],
      "metadata": {
        "id": "S0zJ5oXgYS4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In this project, we implemented a convolutional Variational Autoencoder to learn a latent representation of face images from the UTKFace dataset.\n",
        "The model successfully reconstructs faces and generates new face samples by sampling the latent space.\n",
        "Latent space interpolation demonstrates smooth morphing between different faces, showcasing the meaningful structure learned by the model.\n",
        "This approach highlights the power of VAEs for unsupervised learning and generative modeling of complex image data."
      ],
      "metadata": {
        "id": "jyOFHbbAYMbH"
      }
    }
  ]
}