{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion MNIST Autoencoder & Variational Autoencoder Demo\n",
        "\n",
        "In this notebook, we explore two deep learning architectures for unsupervised learning:\n",
        "- **Autoencoder (AE)**\n",
        "- **Variational Autoencoder (VAE)**\n",
        "\n",
        "Both models are applied to the **Fashion MNIST** dataset using **fully-connected (dense)** layers (no CNNs).\n",
        "\n",
        "Goals:\n",
        "- Compare AE and VAE performance\n",
        "- Visualize reconstructed images\n",
        "- Explore VAE's latent space\n"
      ],
      "metadata": {
        "id": "XRV5r0qIH1vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Setup and Imports**"
      ],
      "metadata": {
        "id": "peTH74DEJIR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZdfIBXcHxr0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model import AutoEncoder, VariationalAutoencoder\n",
        "from utils import (\n",
        "    compute_loss,\n",
        "    train_model,\n",
        "    ae_model_reconstruct_images,\n",
        "    generate_images,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Load & Preprocess Fashion MNIST**"
      ],
      "metadata": {
        "id": "pwEqg1aGI96P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "\n",
        "X_train_full = X_train_full.astype(np.float32) / 255.0\n",
        "X_test = X_test.astype(np.float32) / 255.0\n",
        "\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "batch_size = 64\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(X_train).shuffle(50000).batch(batch_size)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices(X_valid).shuffle(10000).batch(batch_size)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "coding_size = 10\n",
        "num_epochs = 20\n"
      ],
      "metadata": {
        "id": "NfsNTrUXJCb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Train Variational Autoencoder (VAE)**"
      ],
      "metadata": {
        "id": "aVV3upYiIzTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae_model = VariationalAutoencoder(coding_size, input_shape)\n",
        "vae_optimizer = keras.optimizers.Adam(1e-3)\n",
        "sample_latent_vector = tf.random.normal(shape=(16, coding_size))\n",
        "\n",
        "generate_images(vae_model, sample_latent_vector, model_status=\"before\")\n",
        "train_model(vae_model, vae_optimizer, train_ds, valid_ds, num_epochs, sample_latent_vector)\n",
        "generate_images(vae_model, sample_latent_vector, model_status=\"after\")\n"
      ],
      "metadata": {
        "id": "U0n0FkYZIGTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Interpolate Over Latent Space (VAE)**"
      ],
      "metadata": {
        "id": "cHR4trmmIkj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "codings_grid = tf.reshape(sample_latent_vector, (1, 4, 4, coding_size))\n",
        "larger_grid = tf.image.resize(codings_grid, size=[5, 7])\n",
        "interpolated_codings = tf.reshape(larger_grid, [-1, coding_size])\n",
        "images = vae_model.decoder(interpolated_codings).numpy()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i in range(len(images)):\n",
        "    plt.subplot(5, 7, i + 1)\n",
        "    plt.imshow(images[i], cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GPwT3-ziIJWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Explore VAE Latent Dimensions**"
      ],
      "metadata": {
        "id": "muzgB-75IloI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dim in range(coding_size):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
        "    for i, val in enumerate(np.linspace(-3, 3, 10)):\n",
        "        z = np.zeros((1, coding_size))\n",
        "        z[0, dim] = val\n",
        "        img = vae_model.decoder(z)[0].numpy()\n",
        "        axes[i].imshow(img, cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(f'Latent dimension {dim}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nNU5NwAqIORV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Train Autoencoder (AE)**"
      ],
      "metadata": {
        "id": "UVC5rkQhIe6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_model = AutoEncoder(input_shape)\n",
        "ae_model.compile(\n",
        "    loss=keras.losses.MeanSquaredError(),\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "ae_model_reconstruct_images(ae_model, X_train, model_status=\"before\")\n",
        "ae_model.fit(X_train, X_train, epochs=num_epochs, shuffle=True, validation_data=(X_valid, X_valid), verbose=1)\n",
        "ae_model_reconstruct_images(ae_model, X_train, model_status=\"after\")\n"
      ],
      "metadata": {
        "id": "b0SODzEtIQ0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "- **Autoencoder (AE)** effectively learned to reconstruct images, but it's deterministic and lacks generative diversity.\n",
        "- **Variational Autoencoder (VAE)** not only reconstructed images but also:\n",
        "  - Generated new clothing samples from sampled latent vectors\n",
        "  - Showed meaningful variation when altering latent dimensions\n",
        "  - Enabled smooth interpolation in latent space\n",
        "\n",
        "While AE is simpler and performs well for compression, **VAE provides a deeper look into learned representations**, useful for generative modeling and disentanglement analysis.\n",
        "\n",
        "Future work could explore:\n",
        "- Convolutional layers for better spatial encoding\n",
        "- Conditional VAE (CVAE) using class labels\n",
        "- UMAP/t-SNE to visualize latent spaces\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GGvIUT3fITpE"
      }
    }
  ]
}